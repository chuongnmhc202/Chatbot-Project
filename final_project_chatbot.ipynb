{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "kZEGXlFGkkco",
        "4KPPQD4kX5cH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx_3xaDHZL_u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle \n",
        "from tensorflow.keras import layers, activations, models, preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "bfqdNc5OpHGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dialogs.txt\", \"r\") as f:\n",
        "    conversations = []\n",
        "    for line in f:\n",
        "        conversations.append(line.split(\"\\t\"))"
      ],
      "metadata": {
        "id": "g7iLgX0vaVim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [text[0] for text in conversations]\n",
        "answers = [text[1] for text in conversations]"
      ],
      "metadata": {
        "id": "h71gKpvnak2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(questions))\n",
        "print(questions[0])\n",
        "print(f\"Answer: {answers[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "027nFP_8a6fs",
        "outputId": "29456875-730f-4849-a689-2e4e46dcf5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3725\n",
            "hi, how are you doing?\n",
            "Answer: i'm fine. how about yourself?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "eLGptFCba3jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "X3KNxXVjyIrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"<sos> \" + ques.lower().translate(str.maketrans(\"\", \"\", string.punctuation)) + \" <eos>\" for ques in questions]\n",
        "answers = [\"<sos> \" + ans.lower().translate(str.maketrans(\"\", \"\", string.punctuation)) + \" <eos>\" for ans in answers]"
      ],
      "metadata": {
        "id": "rSFSeWe4yeR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qzCI-Y2ByjUX",
        "outputId": "5a1da139-aa37-45b1-f4c6-0f482775e237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<sos> im fine how about yourself <eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**"
      ],
      "metadata": {
        "id": "Ks1GS5CjYtRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = preprocessing.text.Tokenizer(oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocab size: {VOCAB_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WscFnmta2uQ",
        "outputId": "384e7714-6fe1-4d20-b129-018008117907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 2528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index[\"sos\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmYqQtHwx1XJ",
        "outputId": "31da1240-55d5-4848-a809-8403a1b443c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ],
      "metadata": {
        "id": "fyyPkCLqb6zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append(word)"
      ],
      "metadata": {
        "id": "mEC9tmHmbhqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2i = {}\n",
        "i2w = {}\n",
        "for w, i in tokenizer.word_index.items():\n",
        "    w2i[w] = i\n",
        "    i2w[i] = w"
      ],
      "metadata": {
        "id": "7gBe-3olZNku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences):\n",
        "    token_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences: \n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub(\"[^a-zA-Z<>]\", \"\", sentence)\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        token_list.append(tokens)\n",
        "    return token_list, vocabulary"
      ],
      "metadata": {
        "id": "Byx9LX18b2md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "padded_questions = tf.keras.utils.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding=\"post\")\n",
        "encoder_input_data = np.array(padded_questions)"
      ],
      "metadata": {
        "id": "E_YXDBRGcdTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize for answer**"
      ],
      "metadata": {
        "id": "i_-oloEXazoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Input là input sẽ feed vào phần decoder của model\n",
        "\n",
        "Input này được tạo ra từ các câu trả lời trong Dataset, nó sẽ được tokenize, padding về cùng 1 độ dài"
      ],
      "metadata": {
        "id": "OEHlwUWbXhlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "padded_answers = tf.keras.utils.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding=\"post\")\n",
        "decoder_input_data = np.array(padded_answers)"
      ],
      "metadata": {
        "id": "3-lWb4MBdPNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"input data shape: {decoder_input_data.shape}, maxlen: {maxlen_answers}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnBZsJoYdlot",
        "outputId": "0168f32f-d9d0-4d6d-c16e-362f06f04bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input data shape: (3725, 21), maxlen: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder output data**"
      ],
      "metadata": {
        "id": "GoPGOsKDeE8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Output Data cũng tương tự như trên, nhưng dữ liệu sẽ bị cắt bỏ token đầu tiên, các token còn lại chuyển thành dạng One hot vector, để model có thể dự đoán bằng hàm softmax"
      ],
      "metadata": {
        "id": "abgAUWbhX8pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "\n",
        "padded_answers = tf.keras.utils.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding=\"post\")\n",
        "onehot_answers = tf.keras.utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
        "decoder_output_data = np.array(onehot_answers)\n",
        "print(decoder_output_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec9L9y-Bd8Ue",
        "outputId": "4baed5ef-9096-46ab-a110-54281bfe0340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3725, 21, 2528)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get pretrained word embedding**"
      ],
      "metadata": {
        "id": "6nkqiqUeRgb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ở đây, ta sử dụng pretrained word embedding *Glove6B 200 dimensions*"
      ],
      "metadata": {
        "id": "95Nvk7aOYJpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOBh6Du6Ri4E",
        "outputId": "6671465b-1950-4349-f746-c2c9ea121f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get pretrained word embedding glove \n",
        "embedding_index = {}\n",
        "with open(\"/content/drive/MyDrive/DEEP LEARNING/glove.6B.200d.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        word, coef = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coef, \"f\", sep=\" \")\n",
        "        embedding_index[word] = coefs"
      ],
      "metadata": {
        "id": "Iq3GLK6MRnsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(embedding_index))\n",
        "embedding_of_hello = embedding_index[\"hello\"]\n",
        "print(embedding_of_hello.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGBftOqmSPiP",
        "outputId": "a7ac5da0-82ae-4ad6-f20b-c56b8799cf88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000\n",
            "(200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tạo embedding matrix để có thể đưa vào lớp Embedding của model. \n",
        "Embedding matrix có số dòng bằng với số từ vựng, và mỗi dòng là một vector embedding của từ đó"
      ],
      "metadata": {
        "id": "DP1y7jbFYTMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "VKSAUoTBTalG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model"
      ],
      "metadata": {
        "id": "NWiPkMHuWkzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = tf.keras.layers.Input(shape=(maxlen_questions, ))\n",
        "decoder_input = tf.keras.layers.Input(shape=(maxlen_answers, ))"
      ],
      "metadata": {
        "id": "i_D780kYXYjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(input_dim = VOCAB_SIZE,\n",
        "                            output_dim = 200, \n",
        "                            input_length = maxlen_questions,\n",
        "                            weights=[embedding_matrix])"
      ],
      "metadata": {
        "id": "_T9sr_XNXYBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_encoder = embedding_layer(encoder_input)\n",
        "embedding_decoder =  embedding_layer(decoder_input)"
      ],
      "metadata": {
        "id": "X2ZbMBczYqpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_DIM = 200\n",
        "\n",
        "# HIDDEN_DIM = 20\n",
        "encoder_LSTM = tf.keras.layers.LSTM(HIDDEN_DIM, return_state=True)    \n",
        "encoder_outputs, state_h, state_c = encoder_LSTM(embedding_encoder)\n",
        "decoder_LSTM = tf.keras.layers.LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)   \n",
        "decoder_outputs, _, _ = decoder_LSTM(embedding_decoder, initial_state=[state_h, state_c])\n",
        "outputs = tf.keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(decoder_outputs)"
      ],
      "metadata": {
        "id": "advaKAOYWkOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = tf.keras.models.Model([encoder_input, decoder_input], outputs)"
      ],
      "metadata": {
        "id": "5OjVoed2ZB4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "klUBfTd8aBX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Ck1trTZ7t0",
        "outputId": "18d53504-6e6f-4a11-9ae5-2a9b0eb056e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 21)]         0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 21)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 21, 200)      505600      ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  [(None, 200),        320800      ['embedding_2[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, 21, 200),    320800      ['embedding_2[1][0]',            \n",
            "                                 (None, 200),                     'lstm_6[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_6[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 21, 2528)     508128      ['lstm_7[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,655,328\n",
            "Trainable params: 1,655,328\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_output_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwq4dUxWatxg",
        "outputId": "3bd4469f-fcd2-4bc8-ab4f-761a5dc5f36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3725, 21, 2528)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.fit(\n",
        "    [encoder_input_data, decoder_input_data], decoder_output_data,\n",
        "    batch_size=32,\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4sXfX_qZ_Du",
        "outputId": "d3f670bb-ceb9-4d79-cbd4-870c8b7f4593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "117/117 [==============================] - 16s 87ms/step - loss: 2.4330 - accuracy: 0.6742\n",
            "Epoch 2/5\n",
            "117/117 [==============================] - 6s 55ms/step - loss: 1.9516 - accuracy: 0.6992\n",
            "Epoch 3/5\n",
            "117/117 [==============================] - 5s 42ms/step - loss: 1.9139 - accuracy: 0.7002\n",
            "Epoch 4/5\n",
            "117/117 [==============================] - 3s 26ms/step - loss: 1.8780 - accuracy: 0.7020\n",
            "Epoch 5/5\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 1.8434 - accuracy: 0.7049\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31b00a6220>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_respond(text):\n",
        "    tokenized_questions = tokenizer.texts_to_sequences([\"sos \" + text + \" eos\"])\n",
        "    padded_questions = tf.keras.utils.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding=\"post\")\n",
        "    inputs = np.array(padded_questions)\n",
        "\n",
        "    result = ''\n",
        "    dec_input_ids = [[w2i[\"sos\"]]]\n",
        "    \n",
        "    for t in range(maxlen_answers):\n",
        "        dec_input = tf.expand_dims(dec_input_ids, axis=1)\n",
        "        predictions =  model_lstm.predict([inputs, dec_input]) # decoder(dec_input, enc_out, dec_hidden)\n",
        "        predicted_id = tf.argmax(predictions[0][0]).numpy()\n",
        "        if i2w[predicted_id] == \"eos\":\n",
        "            break\n",
        "        result += i2w[predicted_id] + ' '\n",
        "        dec_input_ids = [predicted_id]\n",
        "        \n",
        "    return result"
      ],
      "metadata": {
        "id": "ktWaK_LWbjQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_respond(\"How are you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "ZlCbj_WQbp9R",
        "outputId": "11ac2917-adee-4f63-ad22-d294e7fede88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-35aff035bcc9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_respond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"How are you\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-137-2d8f7238f662>\u001b[0m in \u001b[0;36mgenerate_respond\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# decoder(dec_input, enc_out, dec_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi2w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eos\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/rnn/lstm.py\", line 615, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm_7' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm_7' (type LSTM):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n      • mask=None\n      • training=False\n      • initial_state=['tf.Tensor(shape=(None, 200), dtype=float32)', 'tf.Tensor(shape=(None, 200), dtype=float32)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "kLt1DeL8fnER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, dot, concatenate"
      ],
      "metadata": {
        "id": "lTCYfN7MYw29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "Xq1O315Wbd1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data for this\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(encoder_input_data, padded_answers)"
      ],
      "metadata": {
        "id": "V3yjyS7432Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = X_train.shape[0]\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "hidden_unit = 200\n",
        "embedding_size = 200"
      ],
      "metadata": {
        "id": "zqfQQpGw4WO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "3qdJLbVR4cVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encode(tf.keras.Model):\n",
        "    def __init__(self, embedding_size, vocab_size, hidden_units):\n",
        "        super(Encode, self).__init__()\n",
        "        self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size, weights=[embedding_matrix])\n",
        "        self.GRU = tf.keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform')\n",
        "        self.hidden_units = hidden_units\n",
        "        \n",
        "    def call(self, x, hidden_state):\n",
        "        x = self.Embedding(x)\n",
        "        outputs, last_state = self.GRU(x, hidden_state)\n",
        "        return outputs, last_state\n",
        "    \n",
        "    def init_hidden_state(self, batch_size):\n",
        "        return tf.zeros([batch_size, self.hidden_units])\n"
      ],
      "metadata": {
        "id": "f1uDBf0AbfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W_out_encode = tf.keras.layers.Dense(hidden_units)\n",
        "        self.W_state = tf.keras.layers.Dense(hidden_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, encode_outs, pre_state):\n",
        "        pre_state = tf.expand_dims(pre_state, axis=1)\n",
        "        pre_state = self.W_state(pre_state)\n",
        "        encode_outs = self.W_out_encode(encode_outs)\n",
        "        score = self.V(\n",
        "            tf.nn.tanh(\n",
        "                pre_state + encode_outs)\n",
        "        )\n",
        "        score = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = score*encode_outs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, score\n"
      ],
      "metadata": {
        "id": "cnt5ebdjcDrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QsunCNYYcLDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decode(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_units):\n",
        "        super(Decode, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size, weights=[embedding_matrix])\n",
        "        self.Attention = Attention(hidden_units)\n",
        "        self.GRU = tf.keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "        self.Fc = tf.keras.layers.Dense(vocab_size)\n",
        "            \n",
        "    def call(self, x, encode_outs, pre_state):\n",
        "        x = tf.expand_dims(x, axis=1)\n",
        "        x = self.Embedding(x)\n",
        "        context_vector, attention_weight = self.Attention(encode_outs, pre_state)\n",
        "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
        "        gru_inp = tf.concat([x, context_vector], axis=-1)\n",
        "        out_gru, state = self.GRU(gru_inp)\n",
        "        out_gru = tf.reshape(out_gru, (-1, out_gru.shape[2]))\n",
        "        return self.Fc(out_gru), state\n"
      ],
      "metadata": {
        "id": "zuNg_60dcLoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = 1 - np.equal(real, 0)\n",
        "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "Ft_azScecvoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    \n",
        "EPOCHS = 20\n",
        "optimizer = tf.optimizers.Adam()\n",
        "encoder = Encode(embedding_size, vocab_size=VOCAB_SIZE, hidden_units=hidden_unit)\n",
        "decoder = Decode(vocab_size=VOCAB_SIZE, embedding_size=embedding_size, hidden_units=hidden_unit)\n",
        "    \n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch_id, (x, y) in enumerate(dataset.take(N_BATCH)):\n",
        "        loss = 0\n",
        "        with tf.GradientTape() as tape:\n",
        "            first_state = encoder.init_hidden_state(batch_size=BATCH_SIZE)\n",
        "            encode_outs, last_state = encoder(x, first_state)\n",
        "            decode_state = last_state\n",
        "            decode_input = [w2i[\"sos\"]]*BATCH_SIZE\n",
        "            \n",
        "            for i in range(1, y.shape[1]):\n",
        "                decode_out, decode_state = decoder(\n",
        "                        decode_input, encode_outs, decode_state\n",
        "                )\n",
        "                loss += loss_function(y[:, i], decode_out)\n",
        "                decode_input = y[:, i]\n",
        "                \n",
        "            train_vars = encoder.trainable_variables \\\n",
        "                        + decoder.trainable_variables\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "            optimizer.apply_gradients(zip(grads, train_vars))\n",
        "        total_loss += loss\n",
        "    print(total_loss.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SeOCPakc21H",
        "outputId": "6e7bb8a6-8bf8-49db-d0a6-95e6e25dc13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7ff0100649d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7ff0100649d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3498.7236\n",
            "3295.7292\n",
            "3294.2341\n",
            "3288.8894\n",
            "3251.646\n",
            "3103.4075\n",
            "2965.0376\n",
            "2838.3845\n",
            "2726.757\n",
            "2620.048\n",
            "2522.8022\n",
            "2436.3064\n",
            "2356.0508\n",
            "2276.4597\n",
            "2193.0273\n",
            "2118.3044\n",
            "2051.8335\n",
            "1983.2173\n",
            "1920.2454\n",
            "1836.7229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_respond(text):\n",
        "    tokenized_questions = tokenizer.texts_to_sequences([text])\n",
        "    padded_questions = tf.keras.utils.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding=\"post\")\n",
        "    inputs = np.array(padded_questions)\n",
        "\n",
        "    result = ''\n",
        "    hidden = encoder.init_hidden_state(batch_size=1)\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = [w2i[\"sos\"]]\n",
        "    \n",
        "    for t in range(maxlen_answers):\n",
        "        predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        if i2w[predicted_id] == \"eos\":\n",
        "            break\n",
        "        result += i2w[predicted_id] + ' '\n",
        "        dec_input = [predicted_id]\n",
        "    return result"
      ],
      "metadata": {
        "id": "sZ-JSqJZ7mIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How are you\"\n",
        "\n",
        "print(generate_respond(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63Tu5Q8UAW8H",
        "outputId": "3d5d5121-e9c3-4265-9510-1fa20fa4a6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dont know \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VPEJl7ufcwGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Original**"
      ],
      "metadata": {
        "id": "zp5yQgpeRHEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "witjzDAPfaTc",
        "outputId": "44338c73-bf1a-45bd-8dae-b9e9228f0926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 21)]         0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 21)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 21, 200)      505400      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 21, 200)      505400      ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 200),        320800      ['embedding_3[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 21, 200),    320800      ['embedding_4[0][0]',            \n",
            "                                 (None, 200),                     'lstm_3[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_3[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 21, 2527)     507927      ['lstm_4[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,160,327\n",
            "Trainable params: 2,160,327\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model fail"
      ],
      "metadata": {
        "id": "ukxFJXlQcshm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "Iq4uln2Ug4lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSse1PorTYFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions, ))\n",
        "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True, weights=[embedding_matrix])(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "uf3Fzxd2fom6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention**"
      ],
      "metadata": {
        "id": "TEbL38Ipht7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W_out_encode = tf.keras.layers.Dense(hidden_units)\n",
        "        self.W_state = tf.keras.layers.Dense(hidden_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, encode_outs, pre_state):\n",
        "        pre_state = tf.expand_dims(pre_state, axis=1)\n",
        "        pre_state = self.W_state(pre_state)\n",
        "        encode_outs = self.W_out_encode(encode_outs)\n",
        "        score = self.V(\n",
        "            tf.nn.tanh(\n",
        "                pre_state + encode_outs)\n",
        "        )\n",
        "        score = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = score*encode_outs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, score"
      ],
      "metadata": {
        "id": "vJ74CGBRhxws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**"
      ],
      "metadata": {
        "id": "y8H9e5Jrg6e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True, weights=[embedding_matrix])(decoder_inputs)\n",
        "\n",
        "# context_vector, attention_weights = Attention\n",
        "\n",
        "decoder_outputs , _ , _ = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )(decoder_embedding)\n",
        "outputs = tf.keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(decoder_outputs)"
      ],
      "metadata": {
        "id": "__43jAANgyQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], outputs)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "RaWM8VRthh2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VksDIQ5gj05r",
        "outputId": "9ae273cb-5dca-41f8-b43b-1eba88b7ed86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 21)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 21, 200)      505600      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 21, 200),    320800      ['embedding_1[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 21)]         0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 21, 2528)     508128      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,334,528\n",
            "Trainable params: 1,334,528\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 ) \n",
        "model.save( 'model.h5' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ORkCRNrj2Jn",
        "outputId": "2c71e804-e53f-4a23-a4d8-1d5c5da07444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "75/75 [==============================] - 17s 166ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 2/150\n",
            "75/75 [==============================] - 4s 58ms/step - loss: 7.9536e-04 - accuracy: 1.0000\n",
            "Epoch 3/150\n",
            "75/75 [==============================] - 3s 41ms/step - loss: 5.1081e-04 - accuracy: 1.0000\n",
            "Epoch 4/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 3.6154e-04 - accuracy: 1.0000\n",
            "Epoch 5/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 2.7299e-04 - accuracy: 1.0000\n",
            "Epoch 6/150\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 2.1474e-04 - accuracy: 1.0000\n",
            "Epoch 7/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 1.7421e-04 - accuracy: 1.0000\n",
            "Epoch 8/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 0.0020 - accuracy: 0.9998\n",
            "Epoch 9/150\n",
            "75/75 [==============================] - 2s 32ms/step - loss: 1.3073e-04 - accuracy: 1.0000\n",
            "Epoch 10/150\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 1.0723e-04 - accuracy: 1.0000\n",
            "Epoch 11/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 9.2898e-05 - accuracy: 1.0000\n",
            "Epoch 12/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 8.1768e-05 - accuracy: 1.0000\n",
            "Epoch 13/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 7.2886e-05 - accuracy: 1.0000\n",
            "Epoch 14/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 6.5268e-05 - accuracy: 1.0000\n",
            "Epoch 15/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 5.9021e-05 - accuracy: 1.0000\n",
            "Epoch 16/150\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 5.3609e-05 - accuracy: 1.0000\n",
            "Epoch 17/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 4.8980e-05 - accuracy: 1.0000\n",
            "Epoch 18/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 4.4875e-05 - accuracy: 1.0000\n",
            "Epoch 19/150\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 4.1337e-05 - accuracy: 1.0000\n",
            "Epoch 20/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 3.8130e-05 - accuracy: 1.0000\n",
            "Epoch 21/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 3.5474e-05 - accuracy: 1.0000\n",
            "Epoch 22/150\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 3.2921e-05 - accuracy: 1.0000\n",
            "Epoch 23/150\n",
            "75/75 [==============================] - 2s 32ms/step - loss: 3.0701e-05 - accuracy: 1.0000\n",
            "Epoch 24/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 2.8776e-05 - accuracy: 1.0000\n",
            "Epoch 25/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 2.6923e-05 - accuracy: 1.0000\n",
            "Epoch 26/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 2.5236e-05 - accuracy: 1.0000\n",
            "Epoch 27/150\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 2.3805e-05 - accuracy: 1.0000\n",
            "Epoch 28/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 2.2458e-05 - accuracy: 1.0000\n",
            "Epoch 29/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 2.1196e-05 - accuracy: 1.0000\n",
            "Epoch 30/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 2.0063e-05 - accuracy: 1.0000\n",
            "Epoch 31/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 1.9002e-05 - accuracy: 1.0000\n",
            "Epoch 32/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.8011e-05 - accuracy: 1.0000\n",
            "Epoch 33/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 1.7108e-05 - accuracy: 1.0000\n",
            "Epoch 34/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.6295e-05 - accuracy: 1.0000\n",
            "Epoch 35/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 1.5514e-05 - accuracy: 1.0000\n",
            "Epoch 36/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.4808e-05 - accuracy: 1.0000\n",
            "Epoch 37/150\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 1.4127e-05 - accuracy: 1.0000\n",
            "Epoch 38/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 1.3508e-05 - accuracy: 1.0000\n",
            "Epoch 39/150\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 1.2907e-05 - accuracy: 1.0000\n",
            "Epoch 40/150\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 1.2369e-05 - accuracy: 1.0000\n",
            "Epoch 41/150\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 1.1845e-05 - accuracy: 1.0000\n",
            "Epoch 42/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.1373e-05 - accuracy: 1.0000\n",
            "Epoch 43/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.0916e-05 - accuracy: 1.0000\n",
            "Epoch 44/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 1.0474e-05 - accuracy: 1.0000\n",
            "Epoch 45/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 1.0077e-05 - accuracy: 1.0000\n",
            "Epoch 46/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 9.7080e-06 - accuracy: 1.0000\n",
            "Epoch 47/150\n",
            "75/75 [==============================] - 3s 42ms/step - loss: 9.3497e-06 - accuracy: 1.0000\n",
            "Epoch 48/150\n",
            "75/75 [==============================] - 3s 40ms/step - loss: 8.9771e-06 - accuracy: 1.0000\n",
            "Epoch 49/150\n",
            "75/75 [==============================] - 3s 41ms/step - loss: 8.6518e-06 - accuracy: 1.0000\n",
            "Epoch 50/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 8.3530e-06 - accuracy: 1.0000\n",
            "Epoch 51/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 8.0704e-06 - accuracy: 1.0000\n",
            "Epoch 52/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 7.7901e-06 - accuracy: 1.0000\n",
            "Epoch 53/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 7.5150e-06 - accuracy: 1.0000\n",
            "Epoch 54/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 7.2729e-06 - accuracy: 1.0000\n",
            "Epoch 55/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 7.0352e-06 - accuracy: 1.0000\n",
            "Epoch 56/150\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 6.8112e-06 - accuracy: 1.0000\n",
            "Epoch 57/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 6.5862e-06 - accuracy: 1.0000\n",
            "Epoch 58/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 6.3772e-06 - accuracy: 1.0000\n",
            "Epoch 59/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 6.1850e-06 - accuracy: 1.0000\n",
            "Epoch 60/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 5.9855e-06 - accuracy: 1.0000\n",
            "Epoch 61/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 5.8165e-06 - accuracy: 1.0000\n",
            "Epoch 62/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 5.6465e-06 - accuracy: 1.0000\n",
            "Epoch 63/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 5.4860e-06 - accuracy: 1.0000\n",
            "Epoch 64/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 5.3188e-06 - accuracy: 1.0000\n",
            "Epoch 65/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 5.1683e-06 - accuracy: 1.0000\n",
            "Epoch 66/150\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 5.0187e-06 - accuracy: 1.0000\n",
            "Epoch 67/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 4.8798e-06 - accuracy: 1.0000\n",
            "Epoch 68/150\n",
            "75/75 [==============================] - 3s 41ms/step - loss: 4.7514e-06 - accuracy: 1.0000\n",
            "Epoch 69/150\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 4.6193e-06 - accuracy: 1.0000\n",
            "Epoch 70/150\n",
            "75/75 [==============================] - 2s 30ms/step - loss: 4.4985e-06 - accuracy: 1.0000\n",
            "Epoch 71/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 4.3761e-06 - accuracy: 1.0000\n",
            "Epoch 72/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 4.2634e-06 - accuracy: 1.0000\n",
            "Epoch 73/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 4.1493e-06 - accuracy: 1.0000\n",
            "Epoch 74/150\n",
            "75/75 [==============================] - 3s 37ms/step - loss: 4.0520e-06 - accuracy: 1.0000\n",
            "Epoch 75/150\n",
            "75/75 [==============================] - 3s 35ms/step - loss: 3.9530e-06 - accuracy: 1.0000\n",
            "Epoch 76/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 3.8541e-06 - accuracy: 1.0000\n",
            "Epoch 77/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 3.7604e-06 - accuracy: 1.0000\n",
            "Epoch 78/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 3.6725e-06 - accuracy: 1.0000\n",
            "Epoch 79/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 3.5831e-06 - accuracy: 1.0000\n",
            "Epoch 80/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 3.4991e-06 - accuracy: 1.0000\n",
            "Epoch 81/150\n",
            "75/75 [==============================] - 3s 42ms/step - loss: 3.4183e-06 - accuracy: 1.0000\n",
            "Epoch 82/150\n",
            "75/75 [==============================] - 2s 33ms/step - loss: 3.3372e-06 - accuracy: 1.0000\n",
            "Epoch 83/150\n",
            "75/75 [==============================] - 3s 34ms/step - loss: 3.2582e-06 - accuracy: 1.0000\n",
            "Epoch 84/150\n",
            "75/75 [==============================] - 2s 30ms/step - loss: 3.1890e-06 - accuracy: 1.0000\n",
            "Epoch 85/150\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 3.1188e-06 - accuracy: 1.0000\n",
            "Epoch 86/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 3.0472e-06 - accuracy: 1.0000\n",
            "Epoch 87/150\n",
            "75/75 [==============================] - 3s 40ms/step - loss: 2.9798e-06 - accuracy: 1.0000\n",
            "Epoch 88/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 2.9169e-06 - accuracy: 1.0000\n",
            "Epoch 89/150\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 2.8594e-06 - accuracy: 1.0000\n",
            "Epoch 90/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 2.7981e-06 - accuracy: 1.0000\n",
            "Epoch 91/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 2.7413e-06 - accuracy: 1.0000\n",
            "Epoch 92/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 2.6824e-06 - accuracy: 1.0000\n",
            "Epoch 93/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 2.6318e-06 - accuracy: 1.0000\n",
            "Epoch 94/150\n",
            "75/75 [==============================] - 3s 39ms/step - loss: 2.5763e-06 - accuracy: 1.0000\n",
            "Epoch 95/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 2.5269e-06 - accuracy: 1.0000\n",
            "Epoch 96/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 2.4776e-06 - accuracy: 1.0000\n",
            "Epoch 97/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 2.4334e-06 - accuracy: 1.0000\n",
            "Epoch 98/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 2.3834e-06 - accuracy: 1.0000\n",
            "Epoch 99/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 2.3380e-06 - accuracy: 1.0000\n",
            "Epoch 100/150\n",
            "75/75 [==============================] - 2s 32ms/step - loss: 2.2954e-06 - accuracy: 1.0000\n",
            "Epoch 101/150\n",
            "75/75 [==============================] - 3s 40ms/step - loss: 2.2547e-06 - accuracy: 1.0000\n",
            "Epoch 102/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 2.2140e-06 - accuracy: 1.0000\n",
            "Epoch 103/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 2.1736e-06 - accuracy: 1.0000\n",
            "Epoch 104/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 2.1338e-06 - accuracy: 1.0000\n",
            "Epoch 105/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 2.0994e-06 - accuracy: 1.0000\n",
            "Epoch 106/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 2.0595e-06 - accuracy: 1.0000\n",
            "Epoch 107/150\n",
            "75/75 [==============================] - 4s 49ms/step - loss: 2.0273e-06 - accuracy: 1.0000\n",
            "Epoch 108/150\n",
            "75/75 [==============================] - 2s 28ms/step - loss: 1.9918e-06 - accuracy: 1.0000\n",
            "Epoch 109/150\n",
            "75/75 [==============================] - 2s 33ms/step - loss: 1.9582e-06 - accuracy: 1.0000\n",
            "Epoch 110/150\n",
            "75/75 [==============================] - 2s 31ms/step - loss: 1.9251e-06 - accuracy: 1.0000\n",
            "Epoch 111/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 1.8950e-06 - accuracy: 1.0000\n",
            "Epoch 112/150\n",
            "75/75 [==============================] - 2s 32ms/step - loss: 1.8611e-06 - accuracy: 1.0000\n",
            "Epoch 113/150\n",
            "75/75 [==============================] - 3s 37ms/step - loss: 1.8307e-06 - accuracy: 1.0000\n",
            "Epoch 114/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 1.8055e-06 - accuracy: 1.0000\n",
            "Epoch 115/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.7788e-06 - accuracy: 1.0000\n",
            "Epoch 116/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.7503e-06 - accuracy: 1.0000\n",
            "Epoch 117/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.7238e-06 - accuracy: 1.0000\n",
            "Epoch 118/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.6973e-06 - accuracy: 1.0000\n",
            "Epoch 119/150\n",
            "75/75 [==============================] - 2s 25ms/step - loss: 1.6728e-06 - accuracy: 1.0000\n",
            "Epoch 120/150\n",
            "75/75 [==============================] - 2s 33ms/step - loss: 1.6465e-06 - accuracy: 1.0000\n",
            "Epoch 121/150\n",
            "75/75 [==============================] - 2s 32ms/step - loss: 1.6215e-06 - accuracy: 1.0000\n",
            "Epoch 122/150\n",
            "75/75 [==============================] - 3s 35ms/step - loss: 1.5981e-06 - accuracy: 1.0000\n",
            "Epoch 123/150\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 1.5759e-06 - accuracy: 1.0000\n",
            "Epoch 124/150\n",
            "75/75 [==============================] - 3s 35ms/step - loss: 1.5534e-06 - accuracy: 1.0000\n",
            "Epoch 125/150\n",
            "75/75 [==============================] - 2s 27ms/step - loss: 1.5329e-06 - accuracy: 1.0000\n",
            "Epoch 126/150\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 1.5103e-06 - accuracy: 1.0000\n",
            "Epoch 127/150\n",
            "75/75 [==============================] - 3s 37ms/step - loss: 1.4906e-06 - accuracy: 1.0000\n",
            "Epoch 128/150\n",
            "75/75 [==============================] - 2s 26ms/step - loss: 1.4705e-06 - accuracy: 1.0000\n",
            "Epoch 129/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 1.4512e-06 - accuracy: 1.0000\n",
            "Epoch 130/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.4306e-06 - accuracy: 1.0000\n",
            "Epoch 131/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.4121e-06 - accuracy: 1.0000\n",
            "Epoch 132/150\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 1.3938e-06 - accuracy: 1.0000\n",
            "Epoch 133/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.3768e-06 - accuracy: 1.0000\n",
            "Epoch 134/150\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 1.3575e-06 - accuracy: 1.0000\n",
            "Epoch 135/150\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 1.3429e-06 - accuracy: 1.0000\n",
            "Epoch 136/150\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 1.3241e-06 - accuracy: 1.0000\n",
            "Epoch 137/150\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 1.3064e-06 - accuracy: 1.0000\n",
            "Epoch 138/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.2930e-06 - accuracy: 1.0000\n",
            "Epoch 139/150\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 1.2757e-06 - accuracy: 1.0000\n",
            "Epoch 140/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.2595e-06 - accuracy: 1.0000\n",
            "Epoch 141/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.2441e-06 - accuracy: 1.0000\n",
            "Epoch 142/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.2315e-06 - accuracy: 1.0000\n",
            "Epoch 143/150\n",
            "75/75 [==============================] - 2s 24ms/step - loss: 1.2168e-06 - accuracy: 1.0000\n",
            "Epoch 144/150\n",
            "75/75 [==============================] - 2s 29ms/step - loss: 1.2026e-06 - accuracy: 1.0000\n",
            "Epoch 145/150\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 1.1890e-06 - accuracy: 1.0000\n",
            "Epoch 146/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.1781e-06 - accuracy: 1.0000\n",
            "Epoch 147/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.1623e-06 - accuracy: 1.0000\n",
            "Epoch 148/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.1483e-06 - accuracy: 1.0000\n",
            "Epoch 149/150\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 1.1360e-06 - accuracy: 1.0000\n",
            "Epoch 150/150\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 1.1239e-06 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Talking to the chatbot"
      ],
      "metadata": {
        "id": "kZEGXlFGkkco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    \n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "metadata": {
        "id": "oieM5edOj5fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "  \n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
      ],
      "metadata": {
        "id": "_zAWU3-NknI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ques = input(\"Enter your question: \")\n",
        "tokens = str_to_tokens(ques)"
      ],
      "metadata": {
        "id": "6Yaga2r9vwvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715f2f2f-755d-4300-aa66-a12ad9f4fcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: how are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index['sos']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "039MRiMqxrL7",
        "outputId": "ae8da049-4aef-4726-a3ab-179b6b290f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['sos']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'eos' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "O-ZRTGWIkpVy",
        "outputId": "cde5e884-b6d2-4307-cdbe-137cd226299e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter question : hello\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            " winter patient <oov> <oov> <oov> <oov> <oov> decided birds back birds back elastic about balloon balloon balloon 50 about bed bed crime\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-660bc97be07b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Chatbot using Transformers models"
      ],
      "metadata": {
        "id": "4KPPQD4kX5cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjYtlsVJpp83",
        "outputId": "b465cae6-8e11-4428-95e3-7cdb8b24208c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demo Simple chatbot with BERT**"
      ],
      "metadata": {
        "id": "CgFSnxmBiLoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "LQltMdEskSQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)"
      ],
      "metadata": {
        "id": "MUl2c6O_4oxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
        "                                \"bos_token\": \"<SOS>\",\n",
        "                                \"eos_token\": \"<EOS>\"})"
      ],
      "metadata": {
        "id": "D6yW_9nq476c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r21hGFvZFFv5",
        "outputId": "4919004e-b3d1-41b7-d526-4cb328386632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50258, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "        self.labels = []\n",
        "\n",
        "        self.X = []\n",
        "        for conversation in tqdm(data):\n",
        "            input_text = conversation[0]\n",
        "            label_text = conversation[1]\n",
        "\n",
        "            line = \"<SOS> \" + input_text + \" <bot>: \" + label_text + \" <EOS>\"\n",
        "\n",
        "            \"\"\"input_encoding = self.tokenizer.encode_plus(input_text, add_special_tokens=True, \n",
        "                                                          padding=\"max_length\", truncation=True,\n",
        "                                                          max_length=64, return_tensors=\"pt\")\n",
        "            label_encoding = self.tokenizer.encode_plus(label_text, add_special_tokens=True, \n",
        "                                                          padding=\"max_length\", truncation=True,\n",
        "                                                          max_length=64, return_tensors=\"pt\")\"\"\"\n",
        "            inputs = self.tokenizer.encode_plus(line, add_special_tokens=True, \n",
        "                                                          padding=\"max_length\", truncation=True,\n",
        "                                                          max_length=128, return_tensors=\"pt\")\n",
        "            self.input_ids.append(inputs[\"input_ids\"])     # .append(input_encoding[\"input_ids\"])  \n",
        "            self.attention_masks.append(inputs[\"attention_mask\"])      #.append(input_encoding[\"attention_mask\"])\n",
        "        # self.labels.append(label_encoding[\"input_ids\"])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids) \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"return {\n",
        "            \"input_ids\": self.input_ids[index],\n",
        "            \"attention_mask\": self.attention_masks[index],\n",
        "            \"labels\": self.labels[index]\n",
        "        }\"\"\"\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[index],\n",
        "            \"attention_mask\": self.attention_masks[index]\n",
        "        }"
      ],
      "metadata": {
        "id": "a2J55Of-oH0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ChatbotDataset(conversations, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOk3AI2Encj5",
        "outputId": "7738b90b-42ff-4ba8-f315-6371e2f35c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3725/3725 [00:01<00:00, 3035.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "4bGyjqeg4_Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC5899tr5WqK",
        "outputId": "63dd8a39-b297-4f4b-feca-3184de15cc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        # target_ids = batch['labels'].to(device)\n",
        "        \n",
        "        # Generate the outputs\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "\n",
        "        # outputs = model(input_ids=batch, labels=batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(train_dataset)}\")\n",
        "    \n",
        "# Save model\n",
        "model.save_pretrained(\"gpt2_chatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKAiHnOZpiJO",
        "outputId": "833ec9ee-5a60-4bd2-fc48-5adb23091591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [02:06<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 0.02163321002217747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(inp):\n",
        "    inp = \"<SOS> \"+inp+\" <bot>: \"\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "    output = model.generate(X, attention_mask=a, pad_token_id=50256, max_new_tokens=20)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return output"
      ],
      "metadata": {
        "id": "8RKsLlPWMhco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(\"how are you doing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wbaYNO4YNW_5",
        "outputId": "c18e2db7-d72c-4725-9757-b24a4f0a2533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<SOS> how are you doing <bot>:  i'm doing well.\\n <EOS>SOS> i'm\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P9g4Qa-WWh-4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}